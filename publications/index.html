<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Zhiwei Fang | publications</title>
  <meta name="description" content="Personal website of Zhiwei Fang.">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">

  <!-- Scripts -->
  <script async src="https://badge.dimensions.ai/badge.js" charset="utf-8"></script>
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Zhiwei</span> Fang</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/cv/">
                    cv
                    
                  </a>
              </li>
            
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/projects/">
                    projects
                    
                  </a>
              </li>
            
          
            
              <li class="nav-item navbar-active font-weight-bold">
                  <a class="nav-link" href="/publications/">
                    publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
              </li>
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/teaching/">
                    teaching
                    
                  </a>
              </li>

              <li class="nav-item ">
                  <a class="nav-link" href="/blogs/">
                    blogs
                    
                  </a>
              </li>   
            
          
            
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <!--<div color> for a year's collection-->
  <!--<li> for new paper card, <div abbr> for the short icon long <div> for paper details-->
  <div class="content">

  <h1>publications</h1>
  <h6><span><sup>+</sup></span> denotes equal contribution and joint lead authorship.</h6>


<!-- new item -->
  <div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 70px;" href="http://www.icbir.org/" target="_blank">
          ICBIR
        </a>
        <!-- <a href="/assets/img/pub/AR_LfD_setup.jpg" class="modal-trigger">
          <img src="/assets/img/pub/AR_LfD_setup.jpg" style="width: 70px;" alt="stereotactic OCT for AIS">
        </a> -->


  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="AR_LfD" class="col p-0">
      <h5 class="title mb-0">Augmented Reality Navigation with Optical Coherence Tomography in Robotic Neurosurgery.</h5>
      <div class="author">
                 
              
              
                <nobr><em>Zhiwei Fang</em>,</nobr>


                <nobr>Ruiyang Zhang,</nobr>


                 <nobr>Hok Man Hung,</nobr>



                <nobr>Wu yuan.</nobr>


                  <nobr>Hongliang Ren.</nobr>

        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In  International Conference on Biomimetic Intelligence and Robotics (ICBIR) 
          
          
            2025.
          
        </p>
      </div>

      
    
      <div class="col p-0">

        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#AR_OCT-abstract" role="button" aria-expanded="false" aria-controls="AR_OCT-abstract">Abstract</a>


        <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Website</a>
           
        
        <!-- 
          <div
            class="__dimensions_badge_embed__"
            data-doi="10.1609/aaai.v29i1.9498"
            data-hide-zero-citations="true"
            data-style="small_rectangle"
            data-legend="hover-right"
            style="display: inline-block; margin-bottom: 10px;"
          ></div>
         -->
      </div>
    
      <div class="col mt-2 p-0">
        <div id="AR_OCT-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            An important issue commonly faced in neurosurgical procedures is the lack of Augmented Reality (AR) navigation and perception with sub-millimeter accuracy. The most commonly used magnetic resonance imaging (MRI) scans provide planar imaging of brain structures and corresponding three-dimensional reconstruction results. However, the principles of MRI imaging limit the imaging speed of this modality, and the millimeter-scale resolution is not always satisfactory. Recently, an optical coherence tomography (OCT) imaging method has been applied in neurosurgical procedures due to its higher spatial resolution and imaging speed, and has achieved the goal of deep brain imaging using endoscopic lens designs. However, the limited depth of field and cross-sectional imaging plane of OCT imaging technology often confuse novice users and pose difficulties for real-time understanding during surgical procedures. For this purpose, we explored a method for visualizing OCT images in augmented reality (AR) wearable devices. We collect real-time OCT data through the Robot Operating System (ROS) and perform rendering processing. Then, we develop the AR system interface in Unity software and perform real-time visualization of OCT images. Real-time updating and display of OCT images enable users to observe changes in brain structures during surgical procedures, identify tissue structural content in OCT images, and perform more precise surgical operations.
          </div>
        </div>
      </div>

      
    </div>
  </div>
</div>
</li>
</ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2025</h3>
    </div>
  </div>



<!-- new item -->
  <div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 70px;" href="https://www.iros25.org/" target="_blank">
          IROS
        </a>
        <a href="/assets/img/pub/AR_LfD_setup.jpg" class="modal-trigger">
          <img src="/assets/img/pub/AR_LfD_setup.jpg" style="width: 70px;" alt="AR_LfD IROS">
        </a>


  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="AR_LfD" class="col p-0">
      <h5 class="title mb-0">Head-mounted Robotic Needle Positioning: Learning from Augmented Reality Demonstration of Neuronavigation and Planning.</h5>
      <div class="author">
                 
              
              
                <nobr><em>Zhiwei Fang<sup>+</sup></em>,</nobr>


                 <nobr>Hok Man Hung<sup>+</sup>,</nobr>



                <nobr>Huxin Gao,</nobr>


                  <nobr>Hongliang Ren.</nobr>

        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 
          
          
            2025.
          
        </p>
      </div>

      
    
      <div class="col p-0">

        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#AR_LfD-abstract" role="button" aria-expanded="false" aria-controls="AR_LfD-abstract">Abstract</a>


        <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Website</a>
           
        
        <!-- 
          <div
            class="__dimensions_badge_embed__"
            data-doi="10.1609/aaai.v29i1.9498"
            data-hide-zero-citations="true"
            data-style="small_rectangle"
            data-legend="hover-right"
            style="display: inline-block; margin-bottom: 10px;"
          ></div>
         -->
      </div>
    
      <div class="col mt-2 p-0">
        <div id="AR_LfD-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Robotic needle positioning tasks in neurosurgery often face challenges due to insufficient perception of planar guidance images during surgery. In this work, we propose an Augmented Reality (AR) interface to help perform the robotic needle positioning tasks by learning from demonstration (LfD). Enhanced immersion in the workflow is achieved by displaying surgical scenes and calculated navigation information. The framework utilizes mixed interactive interfaces in virtual and real environments, enhancing demonstration efficiency and quality. A head-mounted display and an optical tracking system are utilized to perform the visualization and needle tracking. Gaussian Mixture Model (GMM) and Gaussian Mixture Regression (GMR) are employed to learn a robust and smooth trajectory policy from demonstrations. Experiments on robot reproduction of the needle positioning task achieved a final positioning error of 0.6 mm and an average trajectory error of 1.07 mm. Comparative user studies with haptic device-based teleoperation exhibit a low completion time of 62.76 s and reduced workload of the proposed system.
          </div>
        </div>
      </div>

      
    </div>
  </div>
</div>
</li>
</ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2025</h3>
    </div>
  </div>

<!-- new item -->
  <div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 70px;" href="https://onlinelibrary.wiley.com/journal/26404567" target="_blank">
          AIS
        </a>
        <a href="/assets/img/pub/AIS.jpg" class="modal-trigger">
          <img src="/assets/img/pub/AIS.jpg" style="width: 80px;" alt="stereotactic OCT for AIS">
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="AIS" class="col p-0">
      <h5 class="title mb-0">Patient-Mounted Neuro Optical Coherence Tomography for Targeted Minimally Invasive Micro-Resolution Volumetric Imaging in Brain In Vivo.</h5>
      <div class="author">
                 
              
                
                  <nobr>Chao Xu<sup>+</sup>,</nobr>
                
              
              
                <nobr><em>Zhiwei Fang<sup>+</sup></em>,</nobr>



                <nobr>Huxin Gao,</nobr>


                  <nobr>Tinghua Zhang,</nobr>


                  <nobr>Tao Zhang,</nobr>


                  <nobr>Peng Liu,</nobr>



                  <nobr>Hongliang Ren*,</nobr>

            
              and
              
                
                  <nobr>Wu Yuan*.</nobr>


          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Advanced Intelligent Systems
          
          
            2024.
          
        </p>
      </div>

      
    
      <div class="col p-0">

        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#NeuroOCT-abstract" role="button" aria-expanded="false" aria-controls="NeuroOCT-abstract">Abstract</a>


        <a class="badge grey waves-effect font-weight-light mr-1" href="https://onlinelibrary.wiley.com/doi/10.1002/aisy.202400488" target="_blank">Website</a>
           
        
        <!-- 
          <div
            class="__dimensions_badge_embed__"
            data-doi="10.1609/aaai.v29i1.9498"
            data-hide-zero-citations="true"
            data-style="small_rectangle"
            data-legend="hover-right"
            style="display: inline-block; margin-bottom: 10px;"
          ></div>
         -->
      </div>
    
      <div class="col mt-2 p-0">
        <div id="NeuroOCT-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Targeted neuroimaging plays a vital role in evaluating pathologies and guiding interventions in deep brain regions, such as biopsy, laser ablation, and deep brain stimulation. However, current neuroimaging techniques face several challenges when it comes to imaging the deep brain. These challenges include limited imaging depth, a narrow field of view, low resolution, and a lack of real-time imaging and stereotactic deployment capabilities. To address these challenges, a patient-mounted neuro optical coherence tomography (neuroOCT) system that combines a lightweight 5 degrees-of-freedom skull-mounted robot (Skullbot) with a neuroendoscope measuring ≈0.6 mm in diameter is introduced. This innovative system enables targeted and minimally invasive neuroimaging with an axial resolution of ≈2.4 μm and a transverse resolution of around 4.5 μm. The Skullbot can be securely attached to the head and precisely deploys the neuroendoscope with an accuracy of ±1.5 mm in the transverse direction and ±0.25 mm in the longitudinal direction. This allows for motion-insensitive stereotactic imaging within the brain. By utilizing the neuroOCT system, targeted imaging of a tumor in a brain phantom is successfully demonstrated. Furthermore, the system's capability for in vivo micro-resolution volumetric neuroimaging of fine structures within a mouse brain is validated.
          </div>
        </div>
      </div>

      
    </div>
  </div>
</div>
</li>
</ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2024</h3>
    </div>
  </div>

<!-- new item -->
<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
  <div class="col-sm-11 p-0">
    <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
<div class="col-sm-1 p-0 abbr">
  
    
      <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 70px;" href="https://iros2024-abudhabi.org/" target="_blank">
        IROS
      </a>
      <a href="/assets/img/pub/IROS.png" class="modal-trigger">
        <img src="/assets/img/pub/IROS.png" style="width: 80px;" alt="head-mounted needle driver">
      </a>
    
  
</div>
<div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
  
  <div id="needle_driver" class="col p-0">
    <h5 class="title mb-0">Head-Mounted Hydraulic Needle Driver for Targeted Interventions in Neurosurgery.</h5>
    <div class="author">
               

              <nobr><em>Zhiwei Fang</em>,</nobr>




              <nobr>Chao Xu,</nobr>



              <nobr>Huxin Gao,</nobr>



              <nobr>Tao Zhang,</nobr>


                <nobr>Tinghua Zhang,</nobr>



                <nobr>Peng Liu,</nobr>


              
                <nobr>Wu Yuan,</nobr>

          
            and


                <nobr>Hongliang Ren.</nobr>


        
      
    </div>

    <div>
      <p class="periodical font-italic">
        
          In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2024.
        
        
      </p>
    </div>

    
  
    <div class="col p-0">

      <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#NeedleDriver-abstract" role="button" aria-expanded="false" aria-controls="NeedleDriver-abstract">Abstract</a>


      <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/10801339" target="_blank">Website</a>
         
      
      <!-- 
        <div
          class="__dimensions_badge_embed__"
          data-doi="10.1609/aaai.v29i1.9498"
          data-hide-zero-citations="true"
          data-style="small_rectangle"
          data-legend="hover-right"
          style="display: inline-block; margin-bottom: 10px;"
        ></div>
       -->
    </div>
  
    <div class="col mt-2 p-0">
      <div id="NeedleDriver-abstract" class="collapse">
        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
          Needle interventions are crucial in neurosurgery, requiring high precision and stability. This paper presents a 5-DoF head-mounted hydraulic needle robot designed for accurate and targeted needle insertion and neuroimaging in the deep brain. The robot is compact and lightweight by utilizing a hydraulic pipe transmission to connect the needle driver and actuator. The syringe pistons serve as the actuator and executor, enabling synchronized motion, minimal hysteresis, and high-accuracy insertion. The hydraulic transmission system exhibits hysteresis of less than 0.8 mm, with bidirectional insertion accuracy of approximately 0.05 mm. The resulting needle driver features a compact structure measuring 48 mm × 25 mm × 9 mm, accompanied by a 70-mm-long needle guide. The needle driver is mainly 3D printed, while the hydraulic transmission ensures full compatibility with magnetic resonance imaging (MRI) by isolating all electromagnetic parts from the executor. This compact and lightweight robot-assisted needle intervention system significantly enhances the safety, accuracy, and effectiveness of deep-brain neuroimaging. The feasibility of precise positioning and insertion is further demonstrated by deploying an optical coherence tomography (OCT) microneedle in a rat brain.
        </div>
      </div>
    </div>

    
  </div>
</div>
</div>
</li>
</ol>
  </div>
  <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
    <h3 class="bibliography-year">2024</h3>
  </div>
</div>

<!-- new item -->
<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 70px;" href="https://spie.org/conferences-and-exhibitions/optics-and-photonics#_=_" target="_blank">
          SPIE
        </a>
        <a href="/assets/img/pub/SPIE.jpg" class="modal-trigger">
          <img src="/assets/img/pub/SPIE.jpg" style="width: 80px;" alt="stereotactic OCT for the SPIE">
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="submilimeter_chao" class="col p-0">
      <h5 class="title mb-0">Submillimeter robotic OCT neuroendoscope for deep-brain imaging in vivo.</h5>
      <div class="author">
                 
              
                
                  <nobr>Chao Xu,</nobr>
                
              
              
                <nobr><em>Zhiwei Fang</em>,</nobr>



                  <nobr>Tinghua Zhang,</nobr>



                  <nobr>Huxin Gao,</nobr>



                  <nobr>Tao Zhang,</nobr>



                  <nobr>Peng Liu,</nobr>



                  <nobr>Hongliang Ren,</nobr>

            
              and
              
                
                  <nobr>Wu Yuan.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Advanced Biomedical and Clinical Diagnostic and Surgical Guidance Systems XXII
          
          
            2024.
          
        </p>
      </div>

      
    
      <div class="col p-0">

        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#submilimeter_chao-abstract" role="button" aria-expanded="false" aria-controls="submilimeter_chao-abstract">Abstract</a>


        <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12831/3000678/Submillimeter-robotic-OCT-neuroendoscope-for-deep-brain-imaging-in-vivo/10.1117/12.3000678.full" target="_blank">Website</a>
           
        
        <!-- 
          <div
            class="__dimensions_badge_embed__"
            data-doi="10.1609/aaai.v29i1.9498"
            data-hide-zero-citations="true"
            data-style="small_rectangle"
            data-legend="hover-right"
            style="display: inline-block; margin-bottom: 10px;"
          ></div>
         -->
      </div>
    
      <div class="col mt-2 p-0">
        <div id="submilimeter_chao-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            To effectively manage inoperable deep-seated brain diseases, a high-resolution diminutive endoscope is required. This endoscope should be capable of precisely localizing and evaluating lesions in vivo. In this study, we introduce an ultrathin robotic OCT neuroendoscope designed for minimally invasive and targeted imaging in the deep brain. The neuroendoscope, measuring only 0.6mm in diameter, is fabricated by coupling a custom micro-lens on the fiber tip. This fabrication technique enables high resolution imaging of 2.4μm x 4.5μm in the axial and transverse directions. To ensure precise trajectory planning and accurate lesion localization within the brain, we have developed a skull-mounted robotic neuroendoscope positioner, allowing for a localization accuracy of approximately 1mm. To demonstrate the capabilities of our technology, we have utilized electromagnetic tracking technology to enable us to control and navigate the neuroendoscope, allowing for the precise localization and imaging of targets within a brain phantom. The new technology holds significant potential to translate OCT neuroendoscopy into clinical practice for deep brain conditions.
          </div>
        </div>
      </div>

      
    </div>
  </div>
</div>
</li>
</ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2024</h3>
    </div>
  </div>


  </div>

  <!-- Footer -->
  <footer>
    &copy; Copyright 2024 Zhiwei Fang.
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- GitHub Stars -->
  <script src="/assets/js/github-stars.js"></script>
  <script type="text/javascript">
    
      
    
      
    
      
    
      
        
        githubStars("eaplatanios/symphony-mt", function(stars) { $("#curriculum-learningeaplatanios-symphony-mt-stars").text('' + stars); });
      
    
      
        
        githubStars("eaplatanios/jelly-bean-world", function(stars) { $("#jelly-bean-worldeaplatanios-jelly-bean-world-stars").text('' + stars); });
      
    
      
        
        githubStars("eaplatanios/symphony-mt", function(stars) { $("#machine-translationeaplatanios-symphony-mt-stars").text('' + stars); });
      
    
      
    
      
    
      
        
        githubStars("eaplatanios/tensorflow_scala", function(stars) { $("#TensorFlow-Scalaeaplatanios-tensorflow_scala-stars").text('' + stars); });
      
    
  </script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-54519238-1', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
